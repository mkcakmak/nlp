{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0111cdd9-f0ac-4c74-98d1-0a102fe4be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re   #Regular Expression\n",
    "\n",
    "reg=r\"Barack\\s(Huesyin\\s)?Obama\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4132bab3-e8e2-475b-87bd-33a1cf00f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11099143382904271360, 0, 3)\n",
      "(11099143382904271360, 12, 15)\n",
      "11099143382904271360 0 3 Good morning,\n",
      "11099143382904271360 12 15 good morning.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Good morning, I want to reservea a ticket.Have a good morning.\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LOWER\":\"good\"},{\"LOWER\":\"morning\"},{\"IS_PUNCT\": True}]\n",
    "\n",
    "matcher.add(\"morningGreeting\",[pattern]) #Each pattern should be a list of dicts,if you only want to add one pattern, make sure to wrap it in a list\n",
    "                                       \n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id in matches:\n",
    "    print(match_id)\n",
    "\n",
    "for match_id,start,end in matches:\n",
    "    m_span = doc[start:end]\n",
    "    print(match_id,start,end,m_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4536d56f-2d0f-478e-a7b9-605df7493133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11099143382904271360, 0, 3)\n",
      "(13992137771284315935, 14, 17)\n",
      "morningGreeting 0 3 GOOD morning,\n",
      "eveningGreeting 14 17 good evening!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"GOOD morning, I want to reserve a ticket. I will then say good evening!\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{\"LOWER\":\"good\"},{\"LOWER\":\"morning\"},{\"IS_PUNCT\":True}]\n",
    "matcher.add(\"morningGreeting\",[pattern1])\n",
    "\n",
    "pattern2 = [{\"LOWER\":\"good\"},{\"LOWER\":\"evening\"},{\"IS_PUNCT\":True}]\n",
    "matcher.add(\"eveningGreeting\", [pattern2])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id in matches:\n",
    "    print(match_id)\n",
    "    \n",
    "for match_id,start,end in matches:\n",
    "    pattern_name = nlp.vocab.strings[match_id]\n",
    "    m_span = doc[start:end]\n",
    "    print(pattern_name,start,end,m_span.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5f29b7-dcbf-4746-a2f6-e50d452f29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 I\n",
      "2 3 a\n",
      "4 5 0\n",
      "5 6 .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I bought a pineapple 0.\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "pattern = [{\"LENGTH\":1}]\n",
    "matcher.add(\"onlyShort\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid,start,end in matches:               # mid : match_id \n",
    "    print(start,end,doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "919153db-8da4-42fa-9b08-c362634ad385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 apples\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"He brought me 2 apples.\")\n",
    "\n",
    "pattern = [{\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]\n",
    "matcher.add(\"numberAndPlainWord\",[pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "mid, start, end = matches[0]\n",
    "print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0add8f05-6f0d-4c00-8016-24e47497221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 SPAM\n",
      "22 23 SUE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp=spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher=Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"Take me out of your SPAM list. We never asked you to contact me. If you write again we'll SUE!!!!\")\n",
    "\n",
    "pattern = [{\"IS_UPPER\": True}]\n",
    "\n",
    "matcher.add(\"capitals\",  [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "\n",
    "     print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b43ebe5-4a28-4ef0-ae20-1642e1780380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Can Sally\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"Can you swim?\")\n",
    "doc2 = nlp(\"Can Sally swim?\")\n",
    "\n",
    "pattern = [{\"IS_SENT_START\": True, \"LOWER\": \"can\"}, {\"IS_TITLE\": True}]\n",
    "\n",
    "matcher.add(\"canThenCapitalized\",  [pattern])\n",
    "\n",
    "matches = matcher(doc2)\n",
    "\n",
    "mid, start, end = matches[0]\n",
    "\n",
    "print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad07e849-b004-4d56-b5cd-d70bb1d6910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Will you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"Will you go there?\")\n",
    "\n",
    "pattern = [{\"IS_SENT_START\":True, \"TAG\":\"MD\"}]  # MD: Modal Verb\n",
    "matcher.add(\"modalVerbStarting\",[pattern])\n",
    "\n",
    "mid,start,end = matches[0]\n",
    "print(start,end,doc1[start:end])\n",
    "\n",
    "doc2 = nlp(\"I might go there\")\n",
    "matcher(doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6457615-7637-475c-9190-96e33c30f179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Standard Consent Form\n",
       "\n",
       "INFORMATION NOTICE ‚Äì TO BE VIEWED AND AGREED PRIOR TO CANDIDATE COMPLETING APPLICATION FORM\n",
       "\n",
       "Please read carefully the Information Notice below. It provides important information to you regarding the collection, use,\n",
       "transfer and storage of your Personal Data for purposes of verifying your background in connection with your role within\n",
       "the organisation identified in your candidate invite (the ‚ÄúRequestor‚Äù).\n",
       "\n",
       "Notification:\n",
       "\n",
       "You have been selected by Requestor via the candidate invite to undergo certain background checks in support of\n",
       "undertaking a role with Requestor and hereby authorise Requestor and its subsidiaries and/or its third party provider\n",
       "HireRight Limited (‚ÄúHireRight‚Äù) and their representatives to: perform reference checks on your employment; verify any\n",
       "information provided through the on boarding process; conduct comprehensive background enquiries including criminal\n",
       "checks and credit checking (if required and permitted by applicable law); and take up personal, academic and employer\n",
       "references (‚ÄúScreening‚Äù). HireRight representatives include (a) subcontractors appointed to support processing of data\n",
       "base checks, academic qualifications and references in particular where only local language sources are available; (b)\n",
       "vendors appointed to support in country credit and criminal checks, where relevant; (c) translation services.\n",
       "\n",
       "In some circumstances the Screening may continue to be made during the course of your employment and the processing\n",
       "and release of any Personal Data obtained as a result of the Screening may be required to be sent to referees, credit\n",
       "agencies, government bodies and other such third parties as may be reasonably necessary in the course of such\n",
       "Screening and during your employment.\n",
       "\n",
       "Information That Will Be Collected and data processor / data controller:\n",
       "\n",
       "During the Screening you will complete an application form where you may be asked to complete the following categories\n",
       "of personal data. The type of data you will complete will be dependent on the Screening being undertaken:\n",
       "\n",
       "‚Ä¢ name;\n",
       "‚Ä¢ age and date of birth;\n",
       "‚Ä¢ place of birth;\n",
       "‚Ä¢ gender;\n",
       "‚Ä¢ your contact details (phone, address, e-mail);\n",
       "‚Ä¢ contact details of referees provided by you (name, phone, address, e-mail, relationship to you);\n",
       "‚Ä¢ education history;\n",
       "‚Ä¢ address history;\n",
       "‚Ä¢ employment history; and\n",
       "‚Ä¢ supporting documents which may include a copy of your passport, ID documentation, certificates showing qualifications (all\n",
       "\n",
       "where lawful to do so);\n",
       "\n",
       "(all ‚ÄúPersonal Data‚Äù).\n",
       "\n",
       "Requestor will then instruct HireRight to collect, research and verify on behalf of Requestor certain information which will\n",
       "depend on the seniority of the position for which you have applied and on national laws. This means [Requestor] will be\n",
       "the data controller in regards to these background checks and HireRight the data processor.\n",
       "\n",
       "Subject to local and regulatory restrictions, this may include:\n",
       "\n",
       "‚Ä¢ Criminal\n",
       "Check\n",
       "\n",
       "Your address history will be used to identify jurisdictions in which you may have resided and gained a criminal\n",
       "conviction history which may have bearing on your suitability to carry out your role. These checks are subject to the\n",
       "availability of information. Requestor has performed a review of the nature of your role and determined that either\n",
       "a Global Criminal Check is relevant and proportionate in context of the functions of your role OR they are permitted\n",
       "to review this information under relevant local laws. Further information as to sources and information returned\n",
       "can be made available on request.\n",
       "\n",
       "‚Ä¢ Credit\n",
       "Check\n",
       "\n",
       "Your address history will be used to identify jurisdictions in which you may have resided and gained a credit history\n",
       "which may have bearing on your suitability to carry out your role. These checks are only available in certain\n",
       "jurisdictions and information may be obtained from a variety of sources. Information returned may include\n",
       "regional court judgments, bankruptcies, voluntary arrangements, adverse financial judgment for debts and\n",
       "negative credit ratings. Requestor has performed a review of the nature of your role and determined that either a\n",
       "Global Credit Check is relevant and proportionate in context of the functions of your role. Further information as to\n",
       "sources and information returned can be made available on request.\n",
       "\n",
       "‚Ä¢ Directorship\n",
       "Search\n",
       "\n",
       "Search of business registries to identify and verify the applicant's current and previously held directorship(s) based\n",
       "on search criteria defined by customer.\n",
       "\n",
       "Further information in respect to verifications\n",
       "\n",
       "You may request a full list of the different checks which will be performed specifically for you from your Requestor. You\n",
       "may also contact your Requestor at any time if you have any questions about the specific information that will be collected\n",
       "about you.\n",
       "\n",
       "HireRight will use appropriate and reliable source to conduct this research and verification.\n",
       "\n",
       "Transfer of Personal Data: outside of the EEA\n",
       "\n",
       "HireRight will, in its capacity as data processor, receive your Personal Data from Requestor and you. HireRight and its\n",
       "representatives will then research, verify and process your Personal Data on behalf of, and in accordance with instructions\n",
       "from, Requestor. To the extent necessary to perform its research and verification, HireRight may provide some of your\n",
       "\n",
       "\n",
       "\n",
       "personal information to third parties. This may include transfers to countries outside the European Economic Area (‚ÄúEEA‚Äù),\n",
       "including to countries that may not be deemed to offer a level of protection for Personal Data as high as countries within\n",
       "the EEA. These third parties are organisations, institutions, agencies or individuals from which information is collected for\n",
       "the purposes of fulfilling the services only and may include local vendors, employers, educational establishments,\n",
       "referees, government agencies, courts, data providers or repositories (‚ÄúSource‚Äù or ‚Äú‚ÄúSources‚Äù) or HireRight‚Äôs\n",
       "representatives (‚ÄúRepresentatives‚Äù) who are performing specific research in connection with your Screening (together\n",
       "‚ÄúThird Parties‚Äù). As contracted with [Requestor], provisions are in place within HireRight in order to ensure an adequate\n",
       "level of data protection for all transfers of Personal Data outside the EEA.\n",
       "\n",
       "In respect to a transfer of your Personal Data to a Source or Sources outside of the EEA, this will be dependent on your\n",
       "footprint during the screening period set by your Requestor. Where your footprint is outside the EEA, your Personal Data\n",
       "will need to be transferred to the relevant Source(s). By way of example, if you have had a period of employment in Hong\n",
       "Kong your previous employer will need to be contacted to verify your job title and dates of employment: to complete the\n",
       "task a data transfer will have to take place. Only information required to obtain the verification will be transferred to the\n",
       "Source(s) and these transfers are to a country in which that Personal Data already exists.\n",
       "\n",
       "HireRight also utilises Representatives located in the Philippines and India. The Representatives are tasked with\n",
       "processing verifications, references and databases searches based on where the candidate has lived previously and to\n",
       "provide support in local geographies and languages by contacting employers, schools and governmental agencies located\n",
       "in APAC during the course of completing the request for a Report. Representatives work from the HireRight Portal with\n",
       "access from a Virtual Desktop Infrastructure which sits on the HireRight UK server network. Representatives are\n",
       "prohibited from copying personal information by any means. This means that all Personal Data will remain within the UK\n",
       "technical environment.\n",
       "\n",
       "HireRight may transfer your Personal Data outside of the United Kingdom to such third parties, if required to complete the\n",
       "verification and that such third parties will provide responses, including Personal Data, requested of them.\n",
       "\n",
       "Support during the Screening process\n",
       "\n",
       "During the course of your Screening you may have questions for the HireRight team or be contacted by the HireRight\n",
       "team to provide further information. For all business hours your HireRight team is located in the EEA at locations in the UK\n",
       "and Poland. If you do contact the HireRight team after business hours your query may be routed to our Representative in\n",
       "the Philippines. You will receive notification of this and you will be provided with the choice of contacting the within\n",
       "business hours team between the hours of 7am to 7pm GMT.\n",
       "\n",
       "Candidate Survey ‚Äì use of personal email address provided for Screening contact\n",
       "\n",
       "HireRight is committed to improving its service and therefore if you contact our Customer Service team you may be\n",
       "randomly selected to participate in a survey to obtain feedback on how HireRight performed. This survey is short, will only\n",
       "take a couple of minutes to complete, and is voluntary. Any contact will be via the personal email address that you\n",
       "provided to HireRight for contact purposes and that email will be stored in SalesForce in the United States. No other\n",
       "personal information will be collected or stored as part of the survey process. HireRight will delete your personal email\n",
       "address from its systems and SalesForce in accordance with the period set out in this Information Notice.\n",
       "\n",
       "Preparation of the report:\n",
       "\n",
       "HireRight will prepare a report that compiles the results of the research and verification and that report will be provided\n",
       "to Requestor (the ‚ÄúReport‚Äù). Requestor may share the Report within its group companies and you should contact\n",
       "Requestor if you have any questions in this regard. The Report will be stored for a period of 12 months on the HireRight\n",
       "servers located in the United Kingdom and will be transferred to Requestor via a secure http portal (the ‚ÄúHireRight Portal‚Äù)\n",
       "using security and technical measures that comply with relevant legislation (including the GDPR and relevant local\n",
       "legislation). The HireRight Portal is ISO27001 certified.\n",
       "\n",
       "Security measures and deletion of Personal Data:\n",
       "\n",
       "HireRight will maintain your Personal Data on a server in the United Kingdom. HireRight is committed to protecting the\n",
       "Personal Data that HireRight receives about individuals and both HireRight and Requestor take measures to secure your\n",
       "Personal Data from accidental loss and from unauthorised access, use, alteration or disclosure. Data is transferred from\n",
       "HireRight securely using encryption and stored on secure servers. Additionally further information security measures are\n",
       "in place, including access controls, physical security and robust information collection, storage and processing practices.\n",
       "HireRight also ensure that where electronic transfer of Personal Data to/from its representatives takes place that such\n",
       "transfers are also appropriately protected and are in compliance with relevant data protection legislation, including the\n",
       "GDPR and in accordance with any instructions provided by a data source.\n",
       "\n",
       "Requestor and HireRight may be required to retain any of your Personal Data for a reasonable period of time in order to\n",
       "comply with legal and regulatory obligations and/or for any other legitimate business purpose.\n",
       "\n",
       "Requestor and HireRight will process your Personal Data in accordance with relevant data privacy legislation.\n",
       "\n",
       "Your Personal Data will be stored for a maximum period of 12 months by HireRight after completion of your Screening\n",
       "after which time my Personal Data will be securely deleted.\n",
       "\n",
       "Candidate Rights\n",
       "\n",
       "You have certain rights arising from privacy legislation in respect to the Personal Data that will be processed in relation to\n",
       "the Screening. Further information is available in respect to such rights at https://www.hireright.com/emea/background-\n",
       "\n",
       "https://www.hireright.com/emea/background-check-faq\n",
       "\n",
       "\n",
       "check-faq but in summary:\n",
       "\n",
       "‚Ä¢ rights of access\n",
       "‚Ä¢ rights of rectification\n",
       "‚Ä¢ right of erasure\n",
       "‚Ä¢ right to object\n",
       "‚Ä¢ right to data portability\n",
       "\n",
       "In each case your rights are exercisable against Requestor and you should direct your requests to SH-HR-\n",
       "ExternalStaffVetting-EMEA@ubs.com.\n",
       "\n",
       "If you have any questions relating to the Screening process you are encouraged to contact Requestor at the address\n",
       "above prior to completing the screening form.\n",
       "\n",
       "It is confirmed that the Screening process does not include any automated decision making or profiling.\n",
       "\n",
       "ATS Integration\n",
       "\n",
       "Requestor utilises Applicant Tracking Systems (ATS) as part of its on-boarding process and the ATS is integrated with the\n",
       "HireRight system to populate certain parts of the screening form you will be asked to complete. Should you have any\n",
       "questions relating to the handling of your Personal Data via the ATS integration then please direct your questions to the\n",
       "relevant ATS provider and Requestor.\n",
       "\n",
       "Legal basis for the processing\n",
       "\n",
       "Requestor and HireRight on behalf of Requestor will process your personal data regarding the above described\n",
       "background screening based on your consent.\n",
       "\n",
       "mailto:SH-HR-ExternalStaffVetting-EMEA@ubs.com\n",
       "\n",
       "\tStandard Consent Form\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content importing from pdf\n",
    "\n",
    "from tika import parser                          # Content importing from pdf\n",
    "\n",
    "parsed_pdf = parser.from_file(\"test.pdf\")       \n",
    "data = parsed_pdf[\"content\"]\n",
    "doc = nlp(data)\n",
    "doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c5853c-559c-4a95-b397-a60ec3434806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n",
      "10 13 good evening!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"Good morning, I'm here. I'll say good evening!!\")\n",
    "\n",
    "pattern = [{\"LOWER\":\"good\"}, {\"LOWER\":{\"IN\": [\"morning\", \"evening\"]}},{\"IS_PUNCT\":True}]\n",
    "\n",
    "matcher.add(\"greetings\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid,start,end in matches:\n",
    "    print(start,end,doc[start:end])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fddf7a-0958-47f0-8261-80b3ed99e82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 Trichotillomania\n",
      "12 13 prescribed\n",
      "14 15 Psychosomatic\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"I suffered from Trichotillomania when I was in college. The doctor prescribed me Psychosomatic medicine.\")\n",
    "\n",
    "pattern = [{\"LENGTH\":{\">=\":10}}]\n",
    "\n",
    "matcher.add(\"longWords\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid,start,end in matches:\n",
    "    print(start,end,doc[start:end])\n",
    "    \n",
    "    \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1380f7a6-88a7-46b6-a0bb-f554e4ae7f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama\n",
      "Barack Hussein Obama\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"Barack Obama visited France\")\n",
    "doc2 = nlp(\"Barack Hussein Obama visited France\")\n",
    "\n",
    "pattern = [{\"LOWER\": \"barack\"}, {\"LOWER\":\"hussein\", \"OP\": \"?\"}, {\"LOWER\":\"obama\"}]\n",
    "\n",
    "matcher.add(\"obamaNames\", [pattern])\n",
    "\n",
    "matches = matcher(doc1)\n",
    "for mid,start,end in matches:\n",
    "    print(doc1[start:end])\n",
    "\n",
    "matches = matcher(doc2)\n",
    "for mid,start,end in matches:\n",
    "    print(doc2[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea206c8-418e-4b34-af65-57df543f1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 hello,\n",
      "1 4 hello hello,\n",
      "0 4 Hello hello hello,\n",
      "\n",
      "\n",
      "0 2 Hello,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "\n",
    "doc3 = nlp(\"How are you?\")\n",
    "\n",
    "pattern = [{\"LOWER\" : {\"IN\" :[\"hello\", \"hi\", \"hallo\"]}, \"OP\": \"+\"}, {\"IS_PUNCT\": True}]     #  + means the token should occur at least once \n",
    "matcher.add(\"greetings\", [pattern])\n",
    "\n",
    "for mid,start,end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "    \n",
    "print(\"\\n\")  \n",
    "\n",
    "for mid,start,end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n",
    "\n",
    "print(\"\\n\")  \n",
    "\n",
    "for mid,start,end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30ec6ae1-e0cf-4b0c-bb18-f3f22b228e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 hello,\n",
      "1 4 hello hello,\n",
      "0 4 Hello hello hello,\n",
      "3 4 ,\n",
      "7 8 ?\n",
      "\n",
      "\n",
      "0 2 Hello,\n",
      "1 2 ,\n",
      "5 6 ?\n",
      "\n",
      "\n",
      "3 4 ?\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "doc3 = nlp(\"How are you?\")\n",
    "\n",
    "pattern = [{\"LOWER\" : {\"IN\" :[\"hello\", \"hi\", \"hallo\"]}, \"OP\": \"*\"}, {\"IS_PUNCT\": True}]     #   * means the token can occur 0 or more times\n",
    "\n",
    "matcher.add(\"greetings\", [pattern])\n",
    "\n",
    "for mid,start,end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "    \n",
    "print(\"\\n\")  \n",
    "\n",
    "for mid,start,end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n",
    "\n",
    "print(\"\\n\")  \n",
    "\n",
    "for mid,start,end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfb37f26-9a2a-4785-b40f-8b8942454eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 name is Alice\n",
      "6 9 name was Elliot\n"
     ]
    }
   ],
   "source": [
    "# wildcard token = any token\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"My name is Alice and his name was Elliot.\")\n",
    "\n",
    "pattern = [{\"LOWER\": \"name\"}, {\"LEMMA\": \"be\"}, {}]       \n",
    "\n",
    "matcher.add(\"pickName\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02191fc6-3dca-4976-9693-6ccd603ad92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 forwarded his message\n",
      "1 4 forwarded many messages\n",
      "1 4 forwarded the message\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"I forwarded his message to you.\")\n",
    "\n",
    "doc2 = nlp(\"I forwarded many messages to you.\")\n",
    "\n",
    "doc3 = nlp(\"I forwarded the message to you.\")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"forward\"},{},{\"LOWER\":{\"IN\" :[\"message\",\"messages\"]}, \"OP\": \"+\"} ]    # Include and optional \n",
    "\n",
    "matcher.add(\"forwardMail\", [pattern])\n",
    "            \n",
    "for mid, start, end in matcher(doc1):\n",
    "            print(start,end, doc1[start:end])\n",
    "                                                    \n",
    "for mid, start, end in matcher(doc2):\n",
    "            print(start,end, doc2[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc3):\n",
    "            print(start,end, doc3[start:end])\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d615fe67-4cd5-4025-85f4-e50f3ea7804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 I travelled\n",
      "0 2 She traveled\n",
      "0 2 We Travelled\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"I travelled by bus\")\n",
    "doc2 = nlp(\"She traveled by bike\")\n",
    "doc3 = nlp(\"We Travelled by train\")\n",
    "matcher.add(\"regex\", [pattern])\n",
    "\n",
    "pattern = [{\"POS\": \"PRON\"}, {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "719eb243-eac8-4b27-b343-25169630d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 went\n",
      "6 7 has\n",
      "7 8 been\n",
      "14 15 has\n",
      "15 16 told\n",
      "18 19 wants\n",
      "20 21 visit\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.\")\n",
    "\n",
    "pattern = [{\"TAG\":{\"REGEX\": \"^V\"}}]              # POS TAGS ^V : VB, VGD, VBG, VBN, VBP, and VBZ\n",
    "\n",
    "matcher.add(\"verbs\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc41e20e-28ac-4d25-8908-d23bf9b91dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 11 Angela Merkel\n",
      "16 18 Donald Trump\n",
      "22 24 Alexis Tsipras\n"
     ]
    }
   ],
   "source": [
    "#¬† PHRASE MATCHER\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "terms = [\"Angela Merkel\", \"Donald Trump\", \"Alexis Tsipras\"]\n",
    "\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "\n",
    "matcher.add(\"politicianList\", patterns)\n",
    "\n",
    "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbfe05c5-d0ff-4ec9-8ca2-f92e4d8fa291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 derivatives\n",
      "6 7 market\n",
      "9 10 asset\n"
     ]
    }
   ],
   "source": [
    "# PHRASE MATCHER\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"LOWER\")\n",
    "\n",
    "terms = [\"asset\", \"investment\", \"Derivatives\", \"Demand\", \"Market\"]\n",
    "\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "\n",
    "matcher.add(\"financialTerms\", patterns)\n",
    "\n",
    "doc = nlp(\"During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb105414-1327-4e9e-ae12-a450c6a7dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 9 192.1.1.1\n",
      "12 13 192.160.1.1\n"
     ]
    }
   ],
   "source": [
    "# IP Matcher\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
    "\n",
    "ip_nums = [\"127.0.0.0\", \"127.256.0.0\"]\n",
    "\n",
    "patterns = [nlp.make_doc(ip) for ip in ip_nums]\n",
    "\n",
    "matcher.add(\"IPNums\", patterns)\n",
    "\n",
    "doc = nlp(\"This log contains the following IP addresses: 192.1.1.1 and 192.12.1.1 and 192.160.1.1 .\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7bac0e1-76df-4c6b-bbd8-3e579c7868e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Bill\n",
      "1 2 Gates\n"
     ]
    }
   ],
   "source": [
    "# Combine Entity Ruler with matcher\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"ENT_TYPE\": \"PERSON\"}]\n",
    "\n",
    "matcher.add(\"personEnt\", [pattern])\n",
    "\n",
    "doc = nlp(\"Bill Gates visited Berlin.\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print( start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee39f8d2-3296-4c8d-84ee-b3e90cc04c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Bill\n",
      "0 2 Bill Gates\n",
      "1 2 Gates\n"
     ]
    }
   ],
   "source": [
    "# Combine Entity Ruler with matcher + Option \n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\":\"+\"}]\n",
    "\n",
    "matcher.add(\"personEnt\", [pattern])\n",
    "\n",
    "doc = nlp(\"Bill Gates visited Berlin.\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print( start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d06192c-2bfd-4f99-8a57-078c834a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 Merkel met\n",
      "3 6 Angela Merkel met\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"ENT_TYPE\": \"PERSON\" , \"OP\":\"+\"},{\"POS\":\"VERB\"}]\n",
    "\n",
    "matcher.add(\"personEntAction\", [pattern])\n",
    "\n",
    "doc = nlp(\"Today German chancellor Angela Merkel met with the US president.\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0432e9b8-2a55-4963-9911-56c5a755ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017,)\n",
      "DATE\n",
      "(chime, 2017)\n",
      "ORG\n"
     ]
    }
   ],
   "source": [
    "# ENTITY RULER\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "doc = nlp(\"I have an acccount with chime since 2017\")\n",
    "\n",
    "print(doc.ents)\n",
    "print(doc[7].ent_type_)\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\" : [{\"LOWER\" : \"chime\"}]}]\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc2 = nlp(\"I have an acccount with chime since 2017\")\n",
    "\n",
    "print(doc2.ents)\n",
    "print(doc2[5].ent_type_)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "840f6c5a-8359-41db-9799-e54093df55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 BE71 0961\n",
      "4 7 BE71 0961 2345\n",
      "4 8 BE71 0961 2345 6769\n",
      "4 6 FR76 3000\n",
      "4 7 FR76 3000 6000\n",
      "4 8 FR76 3000 6000 0112\n",
      "4 9 FR76 3000 6000 0112 3456\n",
      "4 10 FR76 3000 6000 0112 3456 7890\n",
      "4 11 FR76 3000 6000 0112 3456 7890 189\n"
     ]
    }
   ],
   "source": [
    "# Exracting IBAN with Matcher\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load (\"en_core_web_md\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"My IBAN number is BE71 0961 2345 6769, please send the money there.\")\n",
    "\n",
    "doc1 = nlp(\"My IBAN number is FR76 3000 6000 0112 3456 7890 189, please send the money there.\")\n",
    "\n",
    "pattern = [{\"SHAPE\": \"XXdd\"}, {\"TEXT\":{\"REGEX\":\"\\d{1,4}\"},\"OP\":\"+\"}]\n",
    "\n",
    "matcher.add(\"ibanNum\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416116d5-9a7a-4268-a7aa-bfbcc989760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 account number is 8921273\n",
      "1 4 account is 5621254\n"
     ]
    }
   ],
   "source": [
    "# Extract Account Number\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LOWER\": \"account\"},{\"LOWER\": {\"IN\" : [\"number\",\"num\"]}, \"OP\" : \"*\"}, {}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "matcher.add(\"accountNum\", [pattern])\n",
    "\n",
    "doc = nlp(\"My account number is 8921273.\")\n",
    "\n",
    "doc1= nlp(\"My account is 5621254.\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c6acb79-705a-4e93-819f-160ea32c1bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 13 +1 (221) 102-2423\n",
      "7 13 (221) 102-2423\n",
      "---\n",
      "5 10 (221) 102 2423\n"
     ]
    }
   ],
   "source": [
    "# Extract Phone Numbers\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"TEXT\":\"+1\",\"OP\":\"?\"},{\"TEXT\":\"(\"},{\"SHAPE\":\"ddd\"},{\"TEXT\":\")\"},{\"SHAPE\":\"ddd\"},{\"TEXT\":\"-\",\"OP\":\"?\"},{\"SHAPE\":\"dddd\"}]\n",
    "\n",
    "matcher.add(\"telNum\", [pattern])\n",
    "\n",
    "doc1 = nlp(\"You can call my office on +1 (221) 102-2423 or email me directly.\")\n",
    "\n",
    "doc2 = nlp(\"You can call me on (221) 102 2423 or text me.\")\n",
    "\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3df3c13-bb0b-4e99-9b1b-288b6dcf3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 CafeA is very generous\n",
      "0 3 CafeA is horrible\n",
      "0 4 CafeA is terribly expensive\n",
      "0 4 CafeA is pretty amazing\n"
     ]
    }
   ],
   "source": [
    "# Extracting Mentions\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"LOWER\": \"cafea\"}, {\"LEMMA\":\"be\"},{\"POS\":\"ADV\", \"OP\":\"*\"}, {\"POS\":\"ADJ\"}]\n",
    "\n",
    "matcher.add(\"mentions\", [pattern])\n",
    "\n",
    "doc1 = nlp(\"CafeA is very generous with the portions.\")\n",
    "\n",
    "doc2 = nlp(\"CafeA is horrible, we waited for mins for a table.\")\n",
    "\n",
    "doc3 = nlp(\"CafeA is terribly expensive, stay away!\")\n",
    "\n",
    "doc4 = nlp(\"CafeA is pretty amazing, we recommend.\")\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])\n",
    "    \n",
    "for mid, start, end in matcher(doc3):\n",
    "    print(start, end, doc3[start:end])\n",
    "    \n",
    "for mid, start, end in matcher(doc4):\n",
    "    print(start, end, doc4[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78d671b5-f354-43c9-bde7-7a5478afcdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'MySpace']\n",
      "#\n",
      "MySpace\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E178] Each pattern should be a list of dicts, but got: {'TEXT': '#'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('hashtag', [pattern])`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart working out now #WeekendShred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m pattern \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m},{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIS_ASCII\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m}]\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhashtag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mid, start, end \u001b[38;5;129;01min\u001b[39;00m matcher(doc):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(start, end, doc[start:end])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/matcher/matcher.pyx:118\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E178] Each pattern should be a list of dicts, but got: {'TEXT': '#'}. Maybe you accidentally passed a single pattern to Matcher.add instead of a list of patterns? If you only want to add one pattern, make sure to wrap it in a list. For example: `matcher.add('hashtag', [pattern])`"
     ]
    }
   ],
   "source": [
    "# Hashtag Exraction ( Twitter )\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# tokens\n",
    "\n",
    "doc = nlp(\"#MySpace\")\n",
    "    \n",
    "print([token.text for token in doc])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# matcher\n",
    "\n",
    "doc = nlp(\"Start working out now #WeekendShred\")\n",
    "\n",
    "pattern = [{\"TEXT\":\"#\"},{\"IS_ASCII\":True}]\n",
    "\n",
    "matcher.add(\"hashtag\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2828416-528a-474c-bc40-5c1a4c6dc7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 üòç\n",
      "5 6 üò°\n"
     ]
    }
   ],
   "source": [
    "# Exract Emoji\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "pos_emoji = [\"üòÄ\", \"üòÇ\", \"üòä\", \"üòÖ\", \"üôÇ\", \"üòç\"]  \n",
    "\n",
    "neg_emoji = [\"üôÅ\", \"üòî\", \"üòû\", \"üò¢\", \"üò°\", \"üò§\"]\n",
    "\n",
    "pos_patterns = [[{\"ORTH\":emoji}] for emoji in pos_emoji]\n",
    "\n",
    "neg_patterns = [[{\"ORTH\":emoji}] for emoji in neg_emoji]\n",
    "\n",
    "matcher.add(\"posEmoji\", pos_patterns)\n",
    "matcher.add(\"negEmoji\", neg_patterns)     # wƒ±thout brackets\n",
    "\n",
    "doc = nlp(\"I love ZARA üòç\")\n",
    "\n",
    "doc1 = nlp(\"I don't love H&M üò°\")\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "    \n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a53b209-3ef2-4625-adb1-32f7f3f2d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ms.', 'TITLE'), ('Smith', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Expanding Name Entities\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"Ms. Smith left her house 2 hours ago.\")\n",
    "\n",
    "\n",
    "\n",
    "patterns = [{\"label\": \"TITLE\", \"pattern\": [{\"LOWER\": {\"IN\": [\"ms.\", \"mr.\", \"mrs.\", \"prof.\", \"dr.\"]}}]}]\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Ms. Smith left her house\")\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54ec6e-19ce-4733-94f7-24b82da0edf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82454f2fcfdc6787d40960af1e56b917efeebd448828df80da5aec47ffeb793e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
